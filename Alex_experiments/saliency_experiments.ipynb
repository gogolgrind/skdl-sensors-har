{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "\n",
    "img=cv2.imread('Picture1.png') #path to the image\n",
    "img = cv2.resize(img,(int(img.shape[1]),int(img.shape[0])))\n",
    "\n",
    "def edges(img, k, s):\n",
    "    kernel_sharpening = -s * np.ones([k,k])\n",
    "    kernel_sharpening[k//2,k//2] = s * (k**2-1)+1\n",
    "    img = cv2.filter2D(img, -1, kernel_sharpening)\n",
    "    return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        self.xstart = 0\n",
    "        self.xend = 0\n",
    "        self.ystart = 0\n",
    "        self.yend = 0\n",
    "\n",
    "    # Our layer receives two inputs. We need to crop the first input blob\n",
    "    # to match a shape of the second one (keeping batch size and number of channels)\n",
    "    def getMemoryShapes(self, inputs):\n",
    "        inputShape, targetShape = inputs[0], inputs[1]\n",
    "        batchSize, numChannels = inputShape[0], inputShape[1]\n",
    "        height, width = targetShape[2], targetShape[3]\n",
    "\n",
    "        self.ystart = (inputShape[2] - targetShape[2]) // 2\n",
    "        self.xstart = (inputShape[3] - targetShape[3]) // 2\n",
    "        self.yend = self.ystart + height\n",
    "        self.xend = self.xstart + width\n",
    "\n",
    "        return [[batchSize, numChannels, height, width]]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return [inputs[0][:,:,self.ystart:self.yend,self.xstart:self.xend]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.dnn_registerLayer('Crop', CropLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(cv2.samples.findFile(\"deploy.prototxt\"), cv2.samples.findFile(\"hed_pretrained_bsds.caffemodel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = cv2.dnn.blobFromImage(cv2.UMat(img), scalefactor=1, size=(img.shape[1],img.shape[0]), \n",
    "                            mean=(104.00698793, 116.66876762, 122.67891434),\n",
    "                            swapRB=False, crop=False)\n",
    "net.setInput(inp)\n",
    "\n",
    "out = net.forward()\n",
    "out = out[0, 0]\n",
    "mask = np.uint8((out>0.001)*255)\n",
    "#img1 = cv2.merge((img[:,:,0]&mask,img[:,:,1]&mask,img[:,:,2]&mask))\n",
    "showarray(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sharpening = -100 * np.ones([3,3])\n",
    "kernel_sharpening[3//2,3//2] = 100 * (3**2-1)+1\n",
    "print(kernel_sharpening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kh = np.array([[-100,-100,-100],[-100,100*8+1,-100],[-100,-100,-100]])\n",
    "#himg = cv2.filter2D(img,-1,kh)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=0, tileGridSize=(3,3))\n",
    "img1 = img\n",
    "#img1 = cv2.resize(img1,(int(img1.shape[1]*4),int(img1.shape[0]*4)))\n",
    "\n",
    "#img1 = ~cv2.stylization(img1, sigma_s=1, sigma_r=0.0001)\n",
    "#img1 = cv2.edgePreservingFilter(img1, flags=2, sigma_s=200, sigma_r=1)\n",
    "img1 = np.uint8(((img1/255)**(0.5))*255)\n",
    "#img1 = ~cv2.stylization(img1, sigma_s=1, sigma_r=0.0001)\n",
    "#img1 = ~edges(img1,7,-1)\n",
    "#img1 = cv2.edgePreservingFilter(img1, flags=2, sigma_s=200, sigma_r=1)\n",
    "#img1 = ~edges(img1,7,-1)\n",
    "#img1 = cv2.edgePreservingFilter(img1, flags=2, sigma_s=200, sigma_r=1)\n",
    "#img1 = cv2.edgePreservingFilter(img1, flags=1, sigma_s=200, sigma_r=1)\n",
    "#img1 = cv2.merge((clahe.apply(img1[:,:,0]),clahe.apply(img1[:,:,1]),clahe.apply(img1[:,:,2])))\n",
    "#img1 = cv2.resize(img1,(int(img1.shape[1]//4),int(img1.shape[0]//4)))\n",
    "\n",
    "#clahe = cv2.createCLAHE(clipLimit=0, tileGridSize=(7,7))\n",
    "#img1 = cv2.merge((clahe.apply(img1[:,:,0]),clahe.apply(img1[:,:,1]),clahe.apply(img1[:,:,2])))\n",
    "\n",
    "\n",
    "#img1 = np.uint8((img1>0)*255)#edges(img,1,-1)\n",
    "#img1 = edges(img1,7,1)\n",
    "#img1 = cv2.bilateralFilter(img1,3,75,75)\n",
    "\n",
    "\n",
    "\n",
    "#img1 = ~edges(img1,3,1)\n",
    "#img1 = ~edges(img1,7,1)\n",
    "#img1 = ~edges(img1,7,1)\n",
    "#img1 = np.uint8(np.ceil(img1/255)*255)\n",
    "#img1 = img1[:,:,0]|img1[:,:,1]|img1[:,:,2]\n",
    "#img1 = cv2.Canny(img1,0,0)\n",
    "showarray(img1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('Picture1.png') #path to the image\n",
    "img = cv2.resize(img,(int(img.shape[1]),int(img.shape[0])))\n",
    "\n",
    "\n",
    "saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
    "(success, saliencyMap) = saliency.computeSaliency(edges(img,7,1))\n",
    "mask = ~np.uint8(saliencyMap*255)\n",
    "\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "l,a,b = cv2.split(img)\n",
    "clahe = cv2.createCLAHE(clipLimit=0, tileGridSize=(3,3))\n",
    "l = clahe.apply(mask)\n",
    "img = cv2.merge((l,a,b))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "showarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "curr_path = sorted(glob.glob(\"{}/*\".format(\"20190411_day_1_only_smartphone\")))\n",
    "#curr_path = sorted(glob.glob(\"D:\\\\ML\\\\proj\\\\WM_Gasprom\\\\danilovsky\\\\*\"))\n",
    "print(curr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(curr_path[1])\n",
    "print(curr_path[1])\n",
    "\n",
    "def warp(img1, img2, upscale = 4, downscale = 4):\n",
    "    img1_shape = img1.shape\n",
    "    img2_shape = img2.shape\n",
    "    \n",
    "    img1 = cv2.UMat(img1)\n",
    "    img2 = cv2.UMat(img2)\n",
    "    \n",
    "    img1 = cv2.resize(img1,(int(img1_shape[1]*upscale),int(img1_shape[0]*upscale)))\n",
    "    img2 = cv2.resize(img2,(int(img2_shape[1]*upscale),int(img2_shape[0]*upscale)))\n",
    "    \n",
    "    kaze = cv2.AKAZE_create(descriptor_type = cv2.AKAZE_DESCRIPTOR_MLDB)\n",
    "    kaze.getDescriptorType()\n",
    "\n",
    "    kp1, des1 = kaze.detectAndCompute(img1, None)\n",
    "    kp2, des2 = kaze.detectAndCompute(img2, None)\n",
    "    des1 = np.float32(des1.get())\n",
    "    des2 = np.float32(des2.get())\n",
    "    print(des1.shape[0], des2.shape[0])\n",
    "\n",
    "    index_params = dict(algorithm = 1, trees = 1)\n",
    "    search_params = dict(checks = 3)\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)\n",
    "    if(des1.shape[0] > des2.shape[0]):\n",
    "        des1,des2 = des2,des1\n",
    "        kp1,kp2 = kp2,kp1\n",
    "        img1,img2 = img2,img1\n",
    "\n",
    "\n",
    "    matches = matcher.knnMatch(des1, des2, 2)\n",
    "    # store all the good matches as per Lowe's ratio test.\n",
    "    good = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.99*n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good\n",
    "                                      ]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good\n",
    "                                      ]).reshape(-1, 1, 2)\n",
    "    \n",
    "    img1 = cv2.drawKeypoints(img1.get(), [kp1[m.queryIdx] for m in good], None, color=(0,255,0), flags=0)\n",
    "    img2 = cv2.drawKeypoints(img2.get(), [kp2[m.trainIdx] for m in good], None, color=(0,255,0), flags=0)\n",
    "\n",
    "    print(len(good))\n",
    "\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # see https://ch.mathworks.com/help/images/examples/find-image-rotation-and-scale-using-automated-feature-matching.html for details\n",
    "    ss = M[0, 1]\n",
    "    sc = M[0, 0]\n",
    "    scaleRecovered = math.sqrt(ss * ss + sc * sc)\n",
    "    thetaRecovered = math.atan2(ss, sc) * 180 / math.pi\n",
    "    #print(\"MAP: Calculated scale difference: %.2f, \"\n",
    "    #                \"Calculated rotation difference: %.2f\" %\n",
    "    #                (scaleRecovered, thetaRecovered))\n",
    "    \n",
    "    #deskew image\n",
    "    im_out = cv2.warpPerspective(img2, np.linalg.inv(M),\n",
    "        (int(img1_shape[1]*upscale), int(img1_shape[0]*upscale)))\n",
    "    \n",
    "    img1 = cv2.resize(img1,(int(img1_shape[1]/downscale),int(img1_shape[0]/downscale)))\n",
    "    img2 = cv2.resize(img2,(int(img2_shape[1]/downscale),int(img2_shape[0]/downscale)))\n",
    "    im_out = cv2.resize(im_out,(int(img1_shape[1]/downscale),int(img1_shape[0]/downscale)))\n",
    "\n",
    "    return img1, img2, im_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "def canney(img, downscale, final_downscale,low = 0, high = 0):\n",
    "    img_shape = img.shape\n",
    "    img = cv2.resize(img,(int(img_shape[1]//downscale),int(img_shape[0]//downscale)))\n",
    "    img1 = cv2.Canny(img,low,high)\n",
    "    X = img1//255\n",
    "\n",
    "    X = X*convolve2d(X,np.array([[1,1,1], [1,0,1],[1,1,1]]),mode='same')\n",
    "    X = np.uint8((X>0)*(255))\n",
    "    X = cv2.resize(X,(int(img_shape[1]//final_downscale),int(img_shape[0]//final_downscale)))\n",
    "    X = np.uint8((X>1)*(255))\n",
    "    return(X)\n",
    "\n",
    "def pir_canney(img, start_downscale, stop_downscale, final_downscale):\n",
    "    img_shape = img.shape\n",
    "    canney_final = canney(img,start_downscale,stop_downscale)\n",
    "    for dw in range(start_downscale+1,stop_downscale):\n",
    "        canney_final = canney_final//2+canney(img,dw,stop_downscale)//2\n",
    "    \n",
    "    canney_final = cv2.resize(canney_final,(int(img_shape[1]//final_downscale),int(img_shape[0]//final_downscale)))\n",
    "    return(canney_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import imutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "downscale = 2\n",
    "\n",
    "try:\n",
    "    \n",
    "    ret, img = cap.read()\n",
    "    saliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
    "    subtractor0 = cv2.bgsegm.createBackgroundSubtractorCNT(minPixelStability = 20, maxPixelStability = 100, isParallel=True)\n",
    "    #subtractor1 = cv2.bgsegm.createBackgroundSubtractorCNT(minPixelStability = 20, maxPixelStability = 101, isParallel=True)\n",
    "    #subtractor2 = cv2.bgsegm.createBackgroundSubtractorCNT(minPixelStability = 20, maxPixelStability = 101, isParallel=True)\n",
    "    #saliency = cv2.saliency.MotionSaliencyBinWangApr2014_create()\n",
    "    img = cv2.resize(img,(int(img.shape[1]//2),int(img.shape[0]//2)))\n",
    "    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    #saliency.setImagesize(img.shape[1], img.shape[0])\n",
    "    #saliency.init()\n",
    "    while(True):\n",
    "        #img_old = img.copy()\n",
    "        \n",
    "        ret, img = cap.read()\n",
    "        \n",
    "        \n",
    "        \n",
    "        img = cv2.resize(img,(int(img.shape[1]//2),int(img.shape[0]//2)))\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "        (success, saliencyMap) = saliency.computeSaliency(edges(img,3,-1))\n",
    "        \n",
    "        mask = ~np.uint8(saliencyMap*255)\n",
    "\n",
    "\n",
    "        img1 = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l,a,b = cv2.split(img1)\n",
    "        l = mask\n",
    "        \n",
    "        clahe = cv2.createCLAHE(clipLimit=0, tileGridSize=(3,3))\n",
    "        l = clahe.apply(l)\n",
    "        l = subtractor0.apply(l)\n",
    "        img1 = cv2.merge((l,a,b))\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        \n",
    "        \n",
    "        cv2.imshow(\"new\", np.vstack((img,img1,img1)))\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
